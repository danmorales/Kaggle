{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.transform import rescale, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer,MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy,binary_accuracy,sparse_categorical_accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, ReLU, BatchNormalization, GlobalAveragePooling2D,LeakyReLU\n",
    "from keras.optimizers import RMSprop, SGD, Adadelta, Adam, Adagrad, Adamax, Nadam\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path,size1,size2):\n",
    "\n",
    "    im = io.imread(image_path)\n",
    "    im = resize(im, (size1, size2,3), anti_aliasing=True)\n",
    "    img = im.astype(float)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagens(size1,size2):\n",
    "\n",
    "    todas_figuras = []\n",
    "    classe=[]\n",
    "    Path = 'Images/'\n",
    "    \n",
    "    figuras=os.listdir(Path)\n",
    "    for tipo in figuras:\n",
    "        figura=os.listdir(Path+tipo)\n",
    "        for figuras in figura:\n",
    "            todas_figuras.append(preprocess_image(Path+tipo+'/'+figuras,size1,size2))\n",
    "            classe.append(tipo)\n",
    "            \n",
    "    tamanho_pinturas = len(todas_figuras)\n",
    "    tamanho_nomes = len(classe)\n",
    "    print(\"Tamanho do arranjo figuras = \",tamanho_pinturas)\n",
    "    print(\"Tamanho do arranjo nomes = \",tamanho_nomes)\n",
    "    \n",
    "    return np.array(todas_figuras),np.array(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape, nclasses):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=input_shape,name='convo1_bloco1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco1'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\",name='conv2_bloco1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco1'))\n",
    "    model.add(Dropout(0.2,name='Dropout_bloco1'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco1'))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco2'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco2'))\n",
    "    model.add(Dropout(0.2,name='Dropout_bloco2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco2'))\n",
    "    \n",
    "    #model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco3'))\n",
    "    #model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco3'))\n",
    "    #model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco3'))\n",
    "    #model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco3'))\n",
    "    #model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco3'))\n",
    "    \n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco4'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco4'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco4'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco4'))\n",
    "    #model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco4'))\n",
    "    \n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco5'))\n",
    "    #model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco5'))\n",
    "    \n",
    "    model.add(Flatten(name='Flatten'))\n",
    "\n",
    "    model.add(Dense(128, activation='relu',name='Dense_1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization_Dense1'))\n",
    "    model.add(Dropout(0.2,name='Dropout_Dense_1'))\n",
    "    model.add(Dense(128, activation='relu',name='Dense_2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization_Dense2'))\n",
    "    model.add(Dropout(0.2,name='Dropout_Dense_2'))\n",
    "    if(nclasses>2):\n",
    "        model.add(Dense(nclasses , activation='softmax',name='Output_Dense'))\n",
    "    else:\n",
    "        model.add(Dense(1 , activation='sigmoid',name='Output_Dense'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 64\n",
    "size2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando imagens\n",
      "Tamanho do arranjo figuras =  657\n",
      "Tamanho do arranjo nomes =  657\n"
     ]
    }
   ],
   "source": [
    "print(\"Carregando imagens\")\n",
    "imagens, classes = load_imagens(size1,size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, ids = np.unique(classes, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  LabelEncoder()\n",
    "y_classes = encoder.fit_transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imagens, y_classes, test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convo1_bloco1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco1  (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2_bloco1 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco1  (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "Dropout_bloco1 (Dropout)     (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "maxpool_bloco1 (MaxPooling2D (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "convo1_bloco2 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco2  (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "convo2_bloco2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco2  (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "Dropout_bloco2 (Dropout)     (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "maxpool_bloco2 (MaxPooling2D (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "batch_normalization_Dense1 ( (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "Dropout_Dense_1 (Dropout)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_Dense2 ( (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "Dropout_Dense_2 (Dropout)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Output_Dense (Dense)         (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,473,793\n",
      "Trainable params: 4,472,513\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo = createModel(imagens[0].shape, nclasses=n_classes)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'SGD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer = SGD\n"
     ]
    }
   ],
   "source": [
    "if(opt == 'SGD'):\n",
    "    print(\"Optimizer = SGD\")\n",
    "    optimizer = SGD(lr=0.1, decay=1e-2/epochs, momentum=0.95, nesterov=True)\n",
    "elif (opt == 'RMS'):\n",
    "    print(\"Optimizer = RMS\")\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-05, decay=0.0)\n",
    "elif (opt == 'Adadelta'):\n",
    "    print(\"Optimizer = Adadelta\")\n",
    "    optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "elif (opt == 'Adam'):\n",
    "    print(\"Optimizer = Adam\")\n",
    "    optimizer = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-2, decay=1e-5/epochs, amsgrad=True)\n",
    "elif (opt == 'Adagrad'):\n",
    "    print(\"Optimizer = Adagrad\")\n",
    "    optimizer = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "elif (opt == 'Adamax'):\n",
    "    print(\"Optimizer = Adamax\")\n",
    "    optimizer = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "elif (opt == 'NAdam'):\n",
    "    print(\"Optimizer = NAdam\")\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, mode='auto', epsilon=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1='categorical_crossentropy'\n",
    "loss2='sparse_categorical_crossentropy'\n",
    "loss3 = \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer=optimizer, loss=loss3, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 525 samples, validate on 132 samples\n",
      "Epoch 1/500\n",
      "525/525 [==============================] - 56s 106ms/step - loss: 0.7583 - accuracy: 0.6552 - val_loss: 881.2529 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.46970, saving model to weights.hdf5\n",
      "Epoch 2/500\n",
      "250/525 [=============>................] - ETA: 25s - loss: 0.5055 - accuracy: 0.7520"
     ]
    }
   ],
   "source": [
    "history = modelo.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,validation_data=(X_test, y_test), callbacks=[learning_rate_reduction,checkpoint,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
