{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.transform import rescale, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer,MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy,binary_accuracy,sparse_categorical_accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, ReLU, BatchNormalization, GlobalAveragePooling2D,LeakyReLU\n",
    "from keras.optimizers import RMSprop, SGD, Adadelta, Adam, Adagrad, Adamax, Nadam\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path,size1,size2):\n",
    "\n",
    "    im = io.imread(image_path)\n",
    "    im = resize(im, (size1, size2,3), anti_aliasing=True)\n",
    "    img = im.astype(float)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagens(size1,size2):\n",
    "\n",
    "    todas_figuras = []\n",
    "    classe=[]\n",
    "    Path = 'Images/'\n",
    "    \n",
    "    figuras=os.listdir(Path)\n",
    "    for tipo in figuras:\n",
    "        figura=os.listdir(Path+tipo)\n",
    "        for figuras in figura:\n",
    "            todas_figuras.append(preprocess_image(Path+tipo+'/'+figuras,size1,size2))\n",
    "            classe.append(tipo)\n",
    "            \n",
    "    tamanho_pinturas = len(todas_figuras)\n",
    "    tamanho_nomes = len(classe)\n",
    "    print(\"Tamanho do arranjo figuras = \",tamanho_pinturas)\n",
    "    print(\"Tamanho do arranjo nomes = \",tamanho_nomes)\n",
    "    \n",
    "    return np.array(todas_figuras),np.array(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape, nclasses):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=input_shape,name='convo1_bloco1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco1'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\",name='conv2_bloco1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco1'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco1'))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco2'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco2'))\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco3'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco3'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco3'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco3'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco3'))\n",
    "    model.add(BatchNormalization(name='batch_normalization3_bloco3'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco3'))\n",
    "    model.add(BatchNormalization(name='batch_normalization4_bloco3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco3'))\n",
    "    \n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco4'))\n",
    "    model.add(BatchNormalization(name='batch_normalization1_bloco4'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco4'))\n",
    "    model.add(BatchNormalization(name='batch_normalization2_bloco4'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco4'))\n",
    "    model.add(BatchNormalization(name='batch_normalization3_bloco4'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco4'))\n",
    "    model.add(BatchNormalization(name='batch_normalization4_bloco4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco4'))\n",
    "    \n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo1_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo2_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo3_bloco5'))\n",
    "    #model.add(Conv2D(512, (3, 3), activation='relu', padding=\"same\",name='convo4_bloco5'))\n",
    "    #model.add(MaxPooling2D((2, 2), strides=(2, 2),padding='valid',name='maxpool_bloco5'))\n",
    "    \n",
    "    model.add(Flatten(name='Flatten'))\n",
    "\n",
    "    model.add(Dense(512, activation='relu',name='Dense_1'))\n",
    "    model.add(BatchNormalization(name='batch_normalization_Dense1'))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu',name='Dense_2'))\n",
    "    model.add(BatchNormalization(name='batch_normalization_Dense2'))\n",
    "    \n",
    "    if(nclasses>2):\n",
    "        model.add(Dense(nclasses , activation='softmax',name='Output_Dense'))\n",
    "    else:\n",
    "        model.add(Dense(1 , activation='sigmoid',name='Output_Dense'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 64\n",
    "size2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando imagens\n",
      "Tamanho do arranjo figuras =  657\n",
      "Tamanho do arranjo nomes =  657\n"
     ]
    }
   ],
   "source": [
    "print(\"Carregando imagens\")\n",
    "imagens, classes = load_imagens(size1,size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, ids = np.unique(classes, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  LabelEncoder()\n",
    "y_classes = encoder.fit_transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imagens, y_classes, test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convo1_bloco1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco1  (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2_bloco1 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco1  (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "maxpool_bloco1 (MaxPooling2D (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "convo1_bloco2 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco2  (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "convo2_bloco2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco2  (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "maxpool_bloco2 (MaxPooling2D (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "convo1_bloco3 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco3  (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "convo2_bloco3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco3  (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "convo3_bloco3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization3_bloco3  (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "convo4_bloco3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization4_bloco3  (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "maxpool_bloco3 (MaxPooling2D (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "convo1_bloco4 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization1_bloco4  (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "convo2_bloco4 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization2_bloco4  (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "convo3_bloco4 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization3_bloco4  (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "convo4_bloco4 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization4_bloco4  (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "maxpool_bloco4 (MaxPooling2D (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "batch_normalization_Dense1 ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_Dense2 ( (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Output_Dense (Dense)         (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 15,061,057\n",
      "Trainable params: 15,052,097\n",
      "Non-trainable params: 8,960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo = createModel(imagens[0].shape, nclasses=n_classes)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'SGD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer = SGD\n"
     ]
    }
   ],
   "source": [
    "if(opt == 'SGD'):\n",
    "    print(\"Optimizer = SGD\")\n",
    "    optimizer = SGD(lr=0.1, decay=1e-2/epochs, momentum=0.95, nesterov=True)\n",
    "elif (opt == 'RMS'):\n",
    "    print(\"Optimizer = RMS\")\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-05, decay=0.0)\n",
    "elif (opt == 'Adadelta'):\n",
    "    print(\"Optimizer = Adadelta\")\n",
    "    optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "elif (opt == 'Adam'):\n",
    "    print(\"Optimizer = Adam\")\n",
    "    optimizer = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-2, decay=1e-5/epochs, amsgrad=True)\n",
    "elif (opt == 'Adagrad'):\n",
    "    print(\"Optimizer = Adagrad\")\n",
    "    optimizer = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "elif (opt == 'Adamax'):\n",
    "    print(\"Optimizer = Adamax\")\n",
    "    optimizer = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "elif (opt == 'NAdam'):\n",
    "    print(\"Optimizer = NAdam\")\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, mode='auto', epsilon=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1='categorical_crossentropy'\n",
    "loss2='sparse_categorical_crossentropy'\n",
    "loss3 = \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer=optimizer, loss=loss3, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 525 samples, validate on 132 samples\n",
      "Epoch 1/500\n",
      "525/525 [==============================] - 134s 254ms/step - loss: 1.5635 - accuracy: 0.5886 - val_loss: 20206264568.2424 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.46970, saving model to weights.hdf5\n",
      "Epoch 2/500\n",
      "525/525 [==============================] - 138s 263ms/step - loss: 1.8801 - accuracy: 0.5886 - val_loss: 65822984.1818 - val_accuracy: 0.4545\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.46970\n",
      "Epoch 3/500\n",
      "525/525 [==============================] - 140s 267ms/step - loss: 1.1205 - accuracy: 0.6514 - val_loss: 88425724.6061 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.46970\n",
      "Epoch 4/500\n",
      "525/525 [==============================] - 119s 226ms/step - loss: 0.9645 - accuracy: 0.7086 - val_loss: 518046.7273 - val_accuracy: 0.4621\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.46970\n",
      "Epoch 5/500\n",
      "525/525 [==============================] - 129s 246ms/step - loss: 0.8117 - accuracy: 0.7029 - val_loss: 181106.6241 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.46970\n",
      "Epoch 6/500\n",
      "525/525 [==============================] - 129s 245ms/step - loss: 0.5576 - accuracy: 0.7467 - val_loss: 76699.9370 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.46970\n",
      "Epoch 7/500\n",
      "525/525 [==============================] - 135s 257ms/step - loss: 0.5150 - accuracy: 0.7771 - val_loss: 16101.7482 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.46970\n",
      "Epoch 8/500\n",
      "525/525 [==============================] - 134s 256ms/step - loss: 0.4507 - accuracy: 0.7810 - val_loss: 3777.4792 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.46970\n",
      "Epoch 9/500\n",
      "525/525 [==============================] - 132s 252ms/step - loss: 0.3690 - accuracy: 0.8305 - val_loss: 866.5106 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.46970\n",
      "Epoch 10/500\n",
      "525/525 [==============================] - 133s 254ms/step - loss: 0.3819 - accuracy: 0.8305 - val_loss: 289.9569 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.46970\n",
      "Epoch 11/500\n",
      "525/525 [==============================] - 113s 214ms/step - loss: 0.4204 - accuracy: 0.8114 - val_loss: 291.7618 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.46970\n",
      "Epoch 12/500\n",
      "525/525 [==============================] - 111s 211ms/step - loss: 0.3502 - accuracy: 0.8648 - val_loss: 215.5264 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.46970\n",
      "Epoch 13/500\n",
      "525/525 [==============================] - 121s 231ms/step - loss: 0.2922 - accuracy: 0.8819 - val_loss: 112.5457 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.46970\n",
      "Epoch 14/500\n",
      "525/525 [==============================] - 132s 251ms/step - loss: 0.3119 - accuracy: 0.8648 - val_loss: 79.4484 - val_accuracy: 0.4697\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.46970\n",
      "Epoch 15/500\n",
      "525/525 [==============================] - 132s 252ms/step - loss: 0.2633 - accuracy: 0.8914 - val_loss: 46.7211 - val_accuracy: 0.4621\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.46970\n",
      "Epoch 16/500\n",
      "525/525 [==============================] - 126s 239ms/step - loss: 0.3210 - accuracy: 0.8686 - val_loss: 28.3929 - val_accuracy: 0.4621\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.46970\n",
      "Epoch 17/500\n",
      "525/525 [==============================] - 144s 273ms/step - loss: 0.2666 - accuracy: 0.8857 - val_loss: 18.0462 - val_accuracy: 0.4773\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.46970 to 0.47727, saving model to weights.hdf5\n",
      "Epoch 18/500\n",
      "525/525 [==============================] - 130s 247ms/step - loss: 0.2626 - accuracy: 0.8971 - val_loss: 12.5205 - val_accuracy: 0.4848\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.47727 to 0.48485, saving model to weights.hdf5\n",
      "Epoch 19/500\n",
      "525/525 [==============================] - 121s 230ms/step - loss: 0.2716 - accuracy: 0.8819 - val_loss: 9.3098 - val_accuracy: 0.5076\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.48485 to 0.50758, saving model to weights.hdf5\n",
      "Epoch 20/500\n",
      "525/525 [==============================] - 129s 246ms/step - loss: 0.2581 - accuracy: 0.8876 - val_loss: 6.1065 - val_accuracy: 0.5606\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.50758 to 0.56061, saving model to weights.hdf5\n",
      "Epoch 21/500\n",
      "525/525 [==============================] - 121s 231ms/step - loss: 0.2543 - accuracy: 0.9048 - val_loss: 3.7195 - val_accuracy: 0.5985\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.56061 to 0.59848, saving model to weights.hdf5\n",
      "Epoch 22/500\n",
      "525/525 [==============================] - 116s 221ms/step - loss: 0.2420 - accuracy: 0.9010 - val_loss: 2.5106 - val_accuracy: 0.6212\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.59848 to 0.62121, saving model to weights.hdf5\n",
      "Epoch 23/500\n",
      "525/525 [==============================] - 139s 266ms/step - loss: 0.2520 - accuracy: 0.9105 - val_loss: 1.8961 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.62121 to 0.66667, saving model to weights.hdf5\n",
      "Epoch 24/500\n",
      "525/525 [==============================] - 136s 259ms/step - loss: 0.2369 - accuracy: 0.8933 - val_loss: 1.5459 - val_accuracy: 0.7348\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.66667 to 0.73485, saving model to weights.hdf5\n",
      "Epoch 25/500\n",
      "525/525 [==============================] - 139s 264ms/step - loss: 0.2580 - accuracy: 0.9010 - val_loss: 1.2862 - val_accuracy: 0.7348\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.73485\n",
      "Epoch 26/500\n",
      "525/525 [==============================] - 137s 260ms/step - loss: 0.2386 - accuracy: 0.9029 - val_loss: 1.0543 - val_accuracy: 0.7727\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.73485 to 0.77273, saving model to weights.hdf5\n",
      "Epoch 27/500\n",
      "525/525 [==============================] - 128s 244ms/step - loss: 0.2448 - accuracy: 0.9048 - val_loss: 0.8528 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.77273 to 0.79545, saving model to weights.hdf5\n",
      "Epoch 28/500\n",
      "525/525 [==============================] - 129s 246ms/step - loss: 0.2259 - accuracy: 0.9086 - val_loss: 0.7379 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.79545\n",
      "Epoch 29/500\n",
      "525/525 [==============================] - 128s 245ms/step - loss: 0.2252 - accuracy: 0.9048 - val_loss: 0.6648 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.79545\n",
      "Epoch 30/500\n",
      "525/525 [==============================] - 120s 228ms/step - loss: 0.2655 - accuracy: 0.8990 - val_loss: 0.5727 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.79545 to 0.80303, saving model to weights.hdf5\n",
      "Epoch 31/500\n",
      "525/525 [==============================] - 126s 241ms/step - loss: 0.2335 - accuracy: 0.9124 - val_loss: 0.5401 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.80303\n",
      "Epoch 32/500\n",
      "525/525 [==============================] - 136s 259ms/step - loss: 0.2592 - accuracy: 0.8971 - val_loss: 0.5164 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.80303\n",
      "Epoch 33/500\n",
      "525/525 [==============================] - 120s 229ms/step - loss: 0.2422 - accuracy: 0.9067 - val_loss: 0.4858 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.80303\n",
      "Epoch 34/500\n",
      "525/525 [==============================] - 139s 265ms/step - loss: 0.2128 - accuracy: 0.9067 - val_loss: 0.4592 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.80303 to 0.81818, saving model to weights.hdf5\n",
      "Epoch 35/500\n",
      "525/525 [==============================] - 116s 220ms/step - loss: 0.2121 - accuracy: 0.9086 - val_loss: 0.4452 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.81818 to 0.83333, saving model to weights.hdf5\n",
      "Epoch 36/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - 113s 215ms/step - loss: 0.2387 - accuracy: 0.9010 - val_loss: 0.4355 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.83333\n",
      "Epoch 37/500\n",
      "525/525 [==============================] - 113s 216ms/step - loss: 0.2183 - accuracy: 0.9086 - val_loss: 0.4272 - val_accuracy: 0.8409\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.83333 to 0.84091, saving model to weights.hdf5\n",
      "Epoch 38/500\n",
      "525/525 [==============================] - 112s 213ms/step - loss: 0.2122 - accuracy: 0.9124 - val_loss: 0.4223 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.84091\n",
      "Epoch 39/500\n",
      "525/525 [==============================] - 113s 215ms/step - loss: 0.2128 - accuracy: 0.9124 - val_loss: 0.4196 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.84091\n",
      "Epoch 40/500\n",
      "525/525 [==============================] - 127s 242ms/step - loss: 0.2388 - accuracy: 0.8990 - val_loss: 0.4152 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.84091\n",
      "Epoch 41/500\n",
      "525/525 [==============================] - 126s 241ms/step - loss: 0.2044 - accuracy: 0.9162 - val_loss: 0.4072 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.84091\n",
      "Epoch 42/500\n",
      "525/525 [==============================] - 113s 215ms/step - loss: 0.2393 - accuracy: 0.8914 - val_loss: 0.4018 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.84091\n",
      "Epoch 43/500\n",
      "525/525 [==============================] - 112s 213ms/step - loss: 0.2385 - accuracy: 0.9105 - val_loss: 0.3983 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0003906250058207661.\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.84091\n",
      "Epoch 44/500\n",
      "525/525 [==============================] - 112s 214ms/step - loss: 0.2370 - accuracy: 0.9029 - val_loss: 0.3965 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.84091\n",
      "Epoch 45/500\n",
      "525/525 [==============================] - 112s 213ms/step - loss: 0.2141 - accuracy: 0.9200 - val_loss: 0.3959 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.84091\n",
      "Epoch 46/500\n",
      "525/525 [==============================] - 111s 212ms/step - loss: 0.1961 - accuracy: 0.9219 - val_loss: 0.3944 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00019531250291038305.\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.84091\n",
      "Epoch 47/500\n",
      "525/525 [==============================] - 111s 212ms/step - loss: 0.2262 - accuracy: 0.9086 - val_loss: 0.3947 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.84091\n",
      "Epoch 48/500\n",
      "525/525 [==============================] - 111s 212ms/step - loss: 0.2362 - accuracy: 0.8952 - val_loss: 0.3946 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.84091\n",
      "Epoch 49/500\n",
      "525/525 [==============================] - 112s 214ms/step - loss: 0.2052 - accuracy: 0.9181 - val_loss: 0.3944 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765625145519152e-05.\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.84091\n",
      "Epoch 50/500\n",
      "525/525 [==============================] - 111s 211ms/step - loss: 0.2086 - accuracy: 0.9219 - val_loss: 0.3942 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.84091\n",
      "Epoch 51/500\n",
      "525/525 [==============================] - 112s 213ms/step - loss: 0.2091 - accuracy: 0.9181 - val_loss: 0.3942 - val_accuracy: 0.8258\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.84091\n",
      "Epoch 52/500\n",
      "525/525 [==============================] - 130s 247ms/step - loss: 0.1939 - accuracy: 0.9276 - val_loss: 0.3942 - val_accuracy: 0.8258\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 4.882812572759576e-05.\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.84091\n",
      "Epoch 53/500\n",
      "525/525 [==============================] - 129s 245ms/step - loss: 0.2157 - accuracy: 0.9143 - val_loss: 0.3928 - val_accuracy: 0.8258\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.84091\n",
      "Epoch 54/500\n",
      "150/525 [=======>......................] - ETA: 1:21 - loss: 0.1784 - accuracy: 0.9333"
     ]
    }
   ],
   "source": [
    "history = modelo.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,validation_data=(X_test, y_test), callbacks=[learning_rate_reduction,checkpoint,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
